{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEOG0051\n",
    "\n",
    "Mining Social and Geographic Datasets\n",
    "-----------------------------------\n",
    "\n",
    "Coursework Part Two \n",
    "-------------------------------\n",
    "\n",
    "Stephen Law and Nikki Tanu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second task, we would like you to analyse a dataset that contains review data of different venues in the city of Calgary, Canada. With the help of several machine learning techniques that we have learnt in the course, you will be tasked to distill insights from this social media dataset.  Two of its notable features are the geocoding of every reviewed venues and the availability of a considerable amount of text data in it, which lend to its ability to be processed using spatial and text analysis techniques respectively. \n",
    "\n",
    "As a prelude to the analysis prompts below, have a brief think about some of these questions: What can we discover about thevenue review data? Are there any spatial patterns that can be extracted from the data? Can we build a machine learning modelthat predicts review rating for unseen data points using the text of the reviews? Are there certain tendencies in the sentiment ofthe reviews based on certain factors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables contained in the dataset provided in a .csv file, are:\n",
    "\n",
    "* `business_id`, unique identifier of the premise\n",
    "* `Name`, name of premise\n",
    "* `latitude`, `longitude`, i.e. the locational attributes of the venue. <br/>\n",
    "* `reviewcount` or the number of reviews the venue has been given<br/>\n",
    "* `categories` general category of establishment that a venue falls under \n",
    "(Note: this variable is rather messy and might needs some cleaning/consolidation to be usable)<br/>\n",
    "* `hours` or the opening hours of the venue <br/>\n",
    "* `reviewid` unique identifier of the review <br/>\n",
    "* `userid` unique identifier of the individual who left the review<br/>\n",
    "* `starsy` individual ratings of the venue<br/>\n",
    "* `useful`, `funny`, `cool`, `text`, i.e. tags that the user attached to the review<br/>\n",
    "* `date` i.e. the date of the review<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Loading and cleaning the dataset\n",
    "\n",
    "In a realistic context, most text datasets are messy in their raw forms.  They require considerable data cleaning before any analysis can be conducted and, not unlike data cleaning for non-textual datasets, this would include the removal of invalid data, missing values, and outliers. In this first prompt you will be required to complete the tasks stated below to prepare the dataset for subsequent analysis.\n",
    "\n",
    "*  Load and understand the dataset.\n",
    "*  Think about which attributes you will use / focus on (in subsequent prompts) and check its data distribution.\n",
    "*  Pre-process the text review data and create a new column in the data frame which will hold the cleaned review data.\n",
    "*  Some of the steps to consider are: removal of numbers, punctuation, short words, stopwords, lemmatise words, etc. \n",
    "\n",
    "\n",
    "```\n",
    "Example Pipeline\n",
    "for each review in geoDataFrame:\n",
    "    # removes all numbers (hint: re)\n",
    "    # removes all punctuations (hint: re)\n",
    "    # removes short words (hint: re\n",
    "    # tokenize words (hint:nltk)\n",
    "    # removes stopwords (hint: nltk)\n",
    "    # lemmatize (hint: nltk)\n",
    "    # rejoin as sent\n",
    "    # cleantxt = sent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Build a supervised learning model for text analysis\n",
    "The objective of this sub-task is to build a supervised learning model that predicts the star ratings of the venue data, based on the different features of each review included in the dataset. You can choose a subset of venues to review for example based on a general category. You can use a combination of text and non-text features, and below are some guidelines that you could follow:\n",
    "\n",
    "*  Firstly, vectorise the preprocessed review text data to give text features you can used in your model.\n",
    "\n",
    "* Split your dataset into a train and test-set and train K$\\geq$2 machine learning models to predict the star rating varying either features used (E.g. bag of words features vs TF-IDF features) or choice of models.\n",
    "\n",
    "*  Report the model test results.\n",
    "\n",
    "*  Discuss and interpret the results you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Geographic visualisation with geopandas and folium\n",
    "\n",
    "Having explored the dataset, its constituent variables and coverage above, the objective of this sub-task is for you to visualise any of the spatial patterns that emerge from the data that you find interesting. This task is intentionally open-ended and leaves you with some choice. To achieve this, you should:\n",
    "\n",
    "* Choose 1 or 2 variables (including any variables you generated from 3.2.2) that you wish to explore and from the list ofvariables available in the dataset\n",
    "\n",
    "* Use either or both of the geopandas and folium libraries in Python to produce up to 3 visualisations\n",
    "\n",
    "* Comment on the spatial distributions of the 1-2 variables you chose, any trends or outliers that emerge and if they haveany notable implications.\n",
    "\n",
    "* **Note**: You may use any subset of the dataset instead of the entire dataset, but comment on why you chose this subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Extra task (Optional)\n",
    "\n",
    "For extra marks, you could choose 1 of EITHER:\n",
    "\n",
    "(a) Use a pretrained neural word embedding method (ie. `word2vec`) for the supervised learning task and compare the results with the bag of words features, OR,\n",
    "\n",
    "(b) Apply topic modelling (eg. `LDA`) on the text data and give a characterisation of each of the topics that your topic model generates. Comment briefly on whether these characterisations were roughly what you expected before, OR,\n",
    "\n",
    "(c) Run a lexicon-based sentiment analysis (eg. `NLTK Vader Sentiment Analyser`) on the textual data, then report and discuss the results. Does the average lexicon sentiment score for each venue correlate with the average venue ratings provided by the users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
